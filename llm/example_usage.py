"""
LLM ëª¨ë“ˆ ì‚¬ìš© ì˜ˆì œ

ì´ íŒŒì¼ì€ ì¶”ìƒí™” ê³„ì¸µì„ ì‚¬ìš©í•˜ì—¬ Ollamaì™€ OpenAIë¥¼ 
ì–´ë–»ê²Œ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""
import os
from llm import LLMFactory


def example_ollama():
    """Ollama ì‚¬ìš© ì˜ˆì œ"""
    print("\n=== Ollama ì‚¬ìš© ì˜ˆì œ ===")
    
    # Ollama LLM ìƒì„±
    llm = LLMFactory.create(
        provider='ollama',
        model='llama2',
        temperature=0.7
    )
    
    print(f"ìƒì„±ëœ LLM: {llm.__class__.__name__}")
    print(f"ëª¨ë¸ëª…: {llm.get_model_name()}")
    print(f"ì„¤ì •: {llm.get_config()}")
    
    # LangChainê³¼ í•¨ê»˜ ì‚¬ìš©
    from langchain.schema import HumanMessage
    message = HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”!")
    response = llm([message])
    print(f"ì‘ë‹µ: {response.content}")


def example_openai():
    """OpenAI ì‚¬ìš© ì˜ˆì œ"""
    print("\n=== OpenAI ì‚¬ìš© ì˜ˆì œ ===")
    
    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
    if not os.getenv('OPENAI_API_KEY'):
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("export OPENAI_API_KEY='your-api-key' ëª…ë ¹ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        return
    
    # OpenAI LLM ìƒì„±
    llm = LLMFactory.create(
        provider='openai',
        model='gpt-4',
        temperature=0.7,
        max_tokens=100
    )
    
    print(f"ìƒì„±ëœ LLM: {llm.__class__.__name__}")
    print(f"ëª¨ë¸ëª…: {llm.get_model_name()}")
    print(f"ì„¤ì •: {llm.get_config()}")
    
    # LangChainê³¼ í•¨ê»˜ ì‚¬ìš©
    from langchain.schema import HumanMessage
    message = HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”!")
    response = llm([message])
    print(f"ì‘ë‹µ: {response.content}")


def example_with_agent():
    """Agentì™€ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ"""
    print("\n=== Agentì™€ í•¨ê»˜ ì‚¬ìš© ì˜ˆì œ ===")
    
    # í™˜ê²½ë³€ìˆ˜ë‚˜ ì„¤ì •ìœ¼ë¡œ ì œê³µì ì„ íƒ
    provider = os.getenv('LLM_PROVIDER', 'ollama')  # ê¸°ë³¸ê°’: ollama
    model = os.getenv('LLM_MODEL', 'llama2')        # ê¸°ë³¸ê°’: llama2
    
    print(f"ì„ íƒëœ ì œê³µì: {provider}")
    print(f"ì„ íƒëœ ëª¨ë¸: {model}")
    
    # Factoryë¥¼ í†µí•´ LLM ìƒì„±
    if provider == 'openai' and not os.getenv('OPENAI_API_KEY'):
        print("âš ï¸  OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. Ollamaë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        provider = 'ollama'
        model = 'llama2'
    
    llm = LLMFactory.create(provider=provider, model=model)
    
    # MemoryAgentì™€ í•¨ê»˜ ì‚¬ìš©
    from agent.memory_agent import MemoryAgent
    agent = MemoryAgent(llm)
    
    response = agent.chat("íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?")
    print(f"\nAgent ì‘ë‹µ: {response}")


def show_available_providers():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì ëª©ë¡ ì¶œë ¥"""
    print("\n=== ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì ===")
    providers = LLMFactory.get_available_providers()
    for provider in providers:
        print(f"- {provider}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ LLM ì¶”ìƒí™” ê³„ì¸µ ì‚¬ìš© ì˜ˆì œ\n")
    print("SOLID ì›ì¹™ì„ ì ìš©í•œ LLM ëª¨ë“ˆ ë°ëª¨")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì í™•ì¸
    show_available_providers()
    
    # ê° ì œê³µìë³„ ì˜ˆì œ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
    # example_ollama()
    # example_openai()
    
    # Agentì™€ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ
    # example_with_agent()
    
    print("\nâœ… ì˜ˆì œ ì™„ë£Œ!")
    print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
    print("   LLM_PROVIDER=ollama LLM_MODEL=llama2 python llm/example_usage.py")
    print("   LLM_PROVIDER=openai LLM_MODEL=gpt-4 OPENAI_API_KEY=sk-... python llm/example_usage.py")


if __name__ == "__main__":
    main()

