# ğŸ¤– SOLID ê¸°ë°˜ Multi-LLM ì±—ë´‡ í”„ë ˆì„ì›Œí¬

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-green.svg)](https://www.langchain.com/)
[![SOLID](https://img.shields.io/badge/Design-SOLID-orange.svg)](https://en.wikipedia.org/wiki/SOLID)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ ê°œìš”

**SOLID ì›ì¹™**ì„ ì² ì €íˆ ì¤€ìˆ˜í•˜ì—¬ ì„¤ê³„ëœ í™•ì¥ ê°€ëŠ¥í•œ ëŒ€í™”í˜• ì±—ë´‡ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- ğŸ”„ **ë‹¤ì¤‘ LLM ì§€ì›**: Ollama(ë¡œì»¬) / OpenAI(í´ë¼ìš°ë“œ) ê°„ ë™ì  ì „í™˜
- ğŸ­ **Factory íŒ¨í„´**: ëŸ°íƒ€ì„ì— LLM ì œê³µì ì„ íƒ
- ğŸ”Œ **MCP í”„ë¡œí† ì½œ**: Model Context Protocol ê¸°ë°˜ íˆ´ ì„œë²„
- ğŸ§© **ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜**: ëŠìŠ¨í•œ ê²°í•©(Loose Coupling)ê³¼ ë†’ì€ ì‘ì§‘ë„(High Cohesion)
- ğŸ¨ **Streamlit UI**: ì›¹ ê¸°ë°˜ ì¸í„°ë™í‹°ë¸Œ ì¸í„°í˜ì´ìŠ¤

## âœ¨ ì£¼ìš” íŠ¹ì§•

### SOLID ì›ì¹™ ì™„ë²½ ì ìš©
| ì›ì¹™ | ì ìš© ì‚¬ë¡€ |
|------|-----------|
| **S**ingle Responsibility | ê° í´ë˜ìŠ¤ëŠ” í•˜ë‚˜ì˜ ì±…ì„ë§Œ (LLM, Agent, UI ë¶„ë¦¬) |
| **O**pen/Closed | Factoryë¥¼ í†µí•œ í™•ì¥ (ìƒˆ LLM ì¶”ê°€ ì‹œ ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”) |
| **L**iskov Substitution | ëª¨ë“  LLMì€ BaseLLMìœ¼ë¡œ ì¹˜í™˜ ê°€ëŠ¥ |
| **I**nterface Segregation | í´ë¼ì´ì–¸íŠ¸ëŠ” í•„ìš”í•œ ì¸í„°í˜ì´ìŠ¤ë§Œ ì˜ì¡´ |
| **D**ependency Inversion | êµ¬ì²´ í´ë˜ìŠ¤ê°€ ì•„ë‹Œ ì¶”ìƒí™”(BaseLLM)ì— ì˜ì¡´ |

### ì§€ì› LLM ì œê³µì
- âœ… **Ollama**: ë¡œì»¬ LLM (ë¬´ë£Œ, í”„ë¼ì´ë²„ì‹œ ë³´ì¥)
  - llama2, llama3, gemma, mistral ë“±
- âœ… **OpenAI**: í´ë¼ìš°ë“œ LLM (API í‚¤ í•„ìš”)
  - gpt-4, gpt-3.5-turbo ë“±
- ğŸ”œ **í™•ì¥ ê°€ëŠ¥**: Anthropic Claude, Google Gemini ë“± ì¶”ê°€ ê°€ëŠ¥

## í´ë”/íŒŒì¼ êµ¬ì¡°
```
project/
â”œâ”€ llm/
â”‚   â”œâ”€ __init__.py         # ëª¨ë“ˆ ì¸í„°í˜ì´ìŠ¤
â”‚   â”œâ”€ base_llm.py         # LLM ì¶”ìƒ ê¸°ë°˜ í´ë˜ìŠ¤ (ABC)
â”‚   â”œâ”€ ollama.py           # Ollama LLM êµ¬í˜„
â”‚   â”œâ”€ openai_llm.py       # OpenAI LLM êµ¬í˜„
â”‚   â”œâ”€ factory.py          # LLM Factory íŒ¨í„´
â”‚   â””â”€ example_usage.py    # ì‚¬ìš© ì˜ˆì œ
â”œâ”€ agent/
â”‚   â””â”€ memory_agent.py     # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì—ì´ì „íŠ¸
â”œâ”€ ui/
â”‚   â””â”€ streamlit_ui.py     # Streamlit UI
â”œâ”€ mcp-server/
â”‚   â”œâ”€ server/
â”‚   â”‚   â””â”€ main.py         # MCP ì„œë²„ (pay íˆ´)
â”‚   â”œâ”€ client/
â”‚   â”‚   â””â”€ main.py         # MCP í´ë¼ì´ì–¸íŠ¸
â”‚   â””â”€ README.md           # MCP ì‹¤í–‰ ê°€ì´ë“œ
â”œâ”€ main.py                 # ì „ì²´ ì¡°ë¦½ ë° ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”œâ”€ requirements.txt        # ì˜ì¡´ì„± ëª©ë¡
â”œâ”€ RULES.md                # í”„ë¡œì íŠ¸ ê°œë°œ ê·œì¹™(SOLID ë“±)
â””â”€ README.md               # (ì´ ë¬¸ì„œ)
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1ï¸âƒ£ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ LLM ì œê³µì ì„ íƒ ë° ì‹¤í–‰

<details open>
<summary><b>Option A: Ollama ì‚¬ìš© (ë¡œì»¬ LLM, ê¶Œì¥)</b></summary>

**ì¥ì **: ë¬´ë£Œ, ë¹ ë¦„, í”„ë¼ì´ë²„ì‹œ ë³´ì¥

#### ì„¤ì¹˜ (ìµœì´ˆ 1íšŒ)
```bash
# Ollama ì„¤ì¹˜: https://ollama.ai/download

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull gemma3n:e4b
# ë˜ëŠ”
ollama pull llama2
```

#### ì‹¤í–‰
```bash
# í„°ë¯¸ë„ 1: Ollama ì„œë²„ ì‹¤í–‰
ollama serve

# í„°ë¯¸ë„ 2: Streamlit UI ì‹¤í–‰
streamlit run main.py
```

</details>

<details>
<summary><b>Option B: OpenAI ì‚¬ìš© (í´ë¼ìš°ë“œ LLM)</b></summary>

**ì¥ì **: ê°•ë ¥í•œ ì„±ëŠ¥, ì„¤ì¹˜ ë¶ˆí•„ìš”  
**ë‹¨ì **: API í‚¤ í•„ìš”, ë¹„ìš© ë°œìƒ

#### Linux / macOS
```bash
export OPENAI_API_KEY='sk-your-api-key-here'
export LLM_PROVIDER='openai'
export LLM_MODEL='gpt-4'

streamlit run main.py
```

#### Windows (PowerShell)
```powershell
$env:OPENAI_API_KEY='sk-your-api-key-here'
$env:LLM_PROVIDER='openai'
$env:LLM_MODEL='gpt-4'

streamlit run main.py
```

</details>

<details>
<summary><b>Option C: ì™¸ë¶€ vLLM ì„œë²„ ì‚¬ìš©</b></summary>

**ì¥ì **: ì™¸ë¶€ í˜¸ìŠ¤íŒ…ì˜ ì¥ì 

#### í™˜ê²½ ì„¤ì •
```bash
# .env íŒŒì¼ì— vLLM ì„œë²„ URL ì„¤ì •
VLLM_SERVER_URL=https://your-vllm-server-url.com

# Streamlit UI ì‹¤í–‰
streamlit run main.py
```
</details>

### 3ï¸âƒ£ ì›¹ ë¸Œë¼ìš°ì € ì ‘ì†
ë¸Œë¼ìš°ì €ê°€ ìë™ìœ¼ë¡œ ì—´ë¦¬ê±°ë‚˜, ìˆ˜ë™ìœ¼ë¡œ ì ‘ì†:
```
http://localhost:8501
```

### 4ï¸âƒ£ MCP ì„œë²„ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
```bash
cd mcp-server
python client/main.py
```
ğŸ“– ìì„¸í•œ ë‚´ìš©: [mcp-server/README.md](mcp-server/README.md)

---

## ğŸ“¦ í™˜ê²½ë³€ìˆ˜ ì„¤ì •

.env íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. `example.env` íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ í•„ìš”í•œ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

```plaintext
# LLM ê¸°ë³¸ ì„¤ì •
LLM_PROVIDER=ollama
LLM_MODEL=gemma3n:e4b

# OpenAI ì‚¬ìš© ì‹œ
OPENAI_API_KEY=your-openai-api-key-here

# vLLM ì™¸ë¶€ ì„œë²„ ì‚¬ìš© ì‹œ
VLLM_SERVER_URL=https://your-vllm-server-url.com

# LangSmith íŠ¸ë ˆì´ì‹± (ì„ íƒ)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=your-langsmith-api-key-here
LANGCHAIN_PROJECT=test7-local
LANGSMITH_WORKSPACE_ID=your-workspace-id-here
```

> ì°¸ê³ : ì¡°ì§ ë²”ìœ„(org-scoped) LangSmith API í‚¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, `LANGSMITH_WORKSPACE_ID`ê°€ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤. í‚¤ê°€ ì‘ì—…ê³µê°„ ë²”ìœ„(workspace-scoped)ê°€ ì•„ë‹ˆë¼ë©´ 403 Forbiddenì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì¶”ê°€ íŒ:
- UI ë””ë²„ê·¸ ë¡œê·¸ í‘œì‹œë¥¼ ì›í•˜ë©´ `DEBUG_PROMPT=true`ë¥¼ ì„¤ì •í•˜ì„¸ìš”. (ì½˜ì†”ì— í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ ì¶œë ¥)
- vLLM ì„œë²„ê°€ ìì²´ì„œëª… ì¸ì¦ì„œë¼ë©´ `LLMFactory.create('vllm', ..., verify=False)`ë¡œ ì‹¤í–‰í•˜ê±°ë‚˜ ì„œë²„ ì¸ì¦ì„œë¥¼ ì‹ ë¢° ì €ì¥ì†Œì— ì„¤ì¹˜í•˜ì„¸ìš”.

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Streamlit UI                        â”‚
â”‚                   (streamlit_ui.py)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Memory Agent                          â”‚
â”‚                  (memory_agent.py)                       â”‚
â”‚         - ConversationBufferMemory                       â”‚
â”‚         - ConversationChain                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LLM Factory                            â”‚
â”‚                   (factory.py)                           â”‚
â”‚         - create(provider, model)                        â”‚
â”‚         - register(provider, class)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                      â”‚
           â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OllamaLLM      â”‚    â”‚   OpenAILLM      â”‚
â”‚  (ollama.py)     â”‚    â”‚ (openai_llm.py)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                      â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚    BaseLLM       â”‚
           â”‚  (base_llm.py)   â”‚
           â”‚   <<abstract>>   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§­ Mermaid í´ë˜ìŠ¤ êµ¬ì„±ë„

```mermaid
classDiagram
    class BaseLLM {
      - model: str
      - config: dict
      + __call__(...)
      + as_langchain_model()
    }

    class OllamaLLM {
      + _initialize()
      + __call__(...)
    }
    BaseLLM <|-- OllamaLLM

    class OpenAILLM {
      + _initialize()
      + __call__(...)
    }
    BaseLLM <|-- OpenAILLM

    class VLLMLLM {
      + _initialize()
      + __call__(prompt)
    }
    BaseLLM <|-- VLLMLLM

    class LLMFactory {
      + create(provider, model, **kwargs) BaseLLM
      + register(provider, cls)
      + get_available_providers() list~str~
    }

    class MemoryAgent {
      - memory: ConversationBufferMemory
      - chain: ConversationChain
      + chat(user_input) str
    }

    class StreamlitUI {
      + run()
    }

    LLMFactory ..> BaseLLM : creates
    MemoryAgent --> BaseLLM : uses (as LangChain model)
    StreamlitUI ..> MemoryAgent : holds
```

## ğŸ’¡ ê°œë°œ ê·œì¹™

### SOLID ì›ì¹™ ì¤€ìˆ˜
- ìì„¸í•œ ë‚´ìš©: [RULES.md](RULES.md)
- ëª¨ë“  ì½”ë“œ ì„¤ê³„ ë° êµ¬í˜„ ì‹œ SOLID ì›ì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜
- ê° ëª¨ë“ˆ/í´ë˜ìŠ¤/í•¨ìˆ˜ëŠ” **í•˜ë‚˜ì˜ ì±…ì„**ë§Œ ê°–ë„ë¡ ë¶„ë¦¬
- **í™•ì¥ì— ê°œë°©**, **ìˆ˜ì •ì— íì‡„**
- ì¶”ìƒí™”ì™€ ì˜ì¡´ì„± ì—­ì „ì„ ì ê·¹ í™œìš©

### ì½”ë“œ ìŠ¤íƒ€ì¼
- Python PEP 8 ì¤€ìˆ˜
- íƒ€ì… íŒíŠ¸ ì‚¬ìš© ê¶Œì¥
- Docstring ì‘ì„± (Google Style)

## LLM ì¶”ìƒí™” ì•„í‚¤í…ì²˜

### SOLID ì›ì¹™ ì ìš©
```python
# BaseLLM (ì¶”ìƒ ê¸°ë°˜ í´ë˜ìŠ¤) - ì˜ì¡´ì„± ì—­ì „ ì›ì¹™
from llm import LLMFactory

# Factory íŒ¨í„´ìœ¼ë¡œ LLM ìƒì„± - ê°œë°©/íì‡„ ì›ì¹™
llm = LLMFactory.create('ollama', 'llama2')
# ë˜ëŠ”
llm = LLMFactory.create('openai', 'gpt-4', temperature=0.7)

# ëª¨ë“  LLMì€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ - ë¦¬ìŠ¤ì½”í”„ ì¹˜í™˜ ì›ì¹™
response = llm([HumanMessage(content="Hello")])
```

### ìƒˆë¡œìš´ LLM ì œê³µì ì¶”ê°€ ë°©ë²•
```python
from llm import BaseLLM, LLMFactory

class CustomLLM(BaseLLM):
    def _initialize(self):
        # ì´ˆê¸°í™” ë¡œì§
        pass
    
    def __call__(self, *args, **kwargs):
        # í˜¸ì¶œ ë¡œì§
        pass

# ë“±ë¡
LLMFactory.register('custom', CustomLLM)

# ì‚¬ìš©
llm = LLMFactory.create('custom', 'model-name')
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### êµ¬ì¡° í…ŒìŠ¤íŠ¸ (LLM í˜¸ì¶œ ì—†ìŒ)
```bash
python test_llm.py
```

### MCP ì„œë²„ í…ŒìŠ¤íŠ¸
```bash
cd mcp-server
python client/main.py
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

<details>
<summary><b>Ollama ì—°ê²° ì‹¤íŒ¨</b></summary>

**ì¦ìƒ**: `Connection error` ë˜ëŠ” `Failed to connect to Ollama`

**í•´ê²°ë°©ë²•**:
```bash
# Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
ollama serve

# ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
ollama list

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull gemma3n:e4b
```
</details>

<details>
<summary><b>OpenAI API í‚¤ ì˜¤ë¥˜</b></summary>

**ì¦ìƒ**: `AuthenticationError` ë˜ëŠ” `Invalid API key`

**í•´ê²°ë°©ë²•**:
```bash
# API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
echo $OPENAI_API_KEY  # Linux/macOS
echo $env:OPENAI_API_KEY  # Windows PowerShell

# API í‚¤ ì¬ì„¤ì •
export OPENAI_API_KEY='sk-your-correct-api-key'
```
</details>

<details>
<summary><b>íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì˜¤ë¥˜</b></summary>

**ì¦ìƒ**: `ModuleNotFoundError` ë˜ëŠ” ì˜ì¡´ì„± ì¶©ëŒ

**í•´ê²°ë°©ë²•**:
```bash
# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™” (ê¶Œì¥)
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate  # Windows

# íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜
pip install --upgrade pip
pip install -r requirements.txt
```
</details>

<details>
<summary><b>Streamlit í¬íŠ¸ ì¶©ëŒ</b></summary>

**ì¦ìƒ**: `Address already in use: 8501`

**í•´ê²°ë°©ë²•**:
```bash
# ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
streamlit run main.py --server.port 8502
```
</details>

---

## ğŸš€ í™•ì¥ ê°€ì´ë“œ

### ìƒˆë¡œìš´ LLM ì œê³µì ì¶”ê°€
```python
# llm/anthropic_llm.py
from llm import BaseLLM, LLMFactory
from langchain_anthropic import ChatAnthropic

class AnthropicLLM(BaseLLM):
    def _initialize(self):
        self.llm = ChatAnthropic(model=self.model, **self.config)
    
    def __call__(self, *args, **kwargs):
        return self.llm(*args, **kwargs)

# ë“±ë¡
LLMFactory.register('anthropic', AnthropicLLM)
```

### ì»¤ìŠ¤í…€ Agent ì¶”ê°€
```python
# agent/custom_agent.py
from langchain.agents import AgentExecutor

class CustomAgent:
    def __init__(self, llm, tools):
        # Agent ë¡œì§ êµ¬í˜„
        pass
```

### ì¶”ê°€ UI í”„ë ˆì„ì›Œí¬
- **FastAPI**: REST API ì„œë²„ êµ¬ì¶•
- **Gradio**: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
- **Flask**: ê²½ëŸ‰ ì›¹ ì„œë²„

---

## ğŸ“š ê¸°ìˆ  ìŠ¤íƒ

| ì¹´í…Œê³ ë¦¬ | ê¸°ìˆ  | ìš©ë„ |
|---------|------|------|
| **LLM** | Ollama, OpenAI | ë¡œì»¬/í´ë¼ìš°ë“œ ì–¸ì–´ ëª¨ë¸ |
| **Framework** | LangChain | LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ |
| **Protocol** | MCP | Model Context Protocol |
| **UI** | Streamlit | ì›¹ ì¸í„°í˜ì´ìŠ¤ |
| **Pattern** | Factory, ABC | ë””ìì¸ íŒ¨í„´ |
| **Architecture** | SOLID | ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ê³„ ì›ì¹™ |

---

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License - ììœ ë¡­ê²Œ ì‚¬ìš©, ìˆ˜ì •, ë°°í¬ ê°€ëŠ¥

---

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

ì´ìŠˆ, PR, í”¼ë“œë°± í™˜ì˜í•©ë‹ˆë‹¤!

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

**Made with â¤ï¸ using SOLID principles** 