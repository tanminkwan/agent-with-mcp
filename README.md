# π¤– SOLID κΈ°λ° Multi-LLM μ±—λ΄‡ ν”„λ μ„μ›ν¬

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-green.svg)](https://www.langchain.com/)
[![SOLID](https://img.shields.io/badge/Design-SOLID-orange.svg)](https://en.wikipedia.org/wiki/SOLID)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## π“‹ κ°μ”

**SOLID μ›μΉ™**μ„ μ² μ €ν μ¤€μν•μ—¬ μ„¤κ³„λ ν™•μ¥ κ°€λ¥ν• λ€ν™”ν• μ±—λ΄‡ ν”„λ μ„μ›ν¬μ…λ‹λ‹¤.

ν•µμ‹¬ κΈ°λ¥:
- π”„ **λ‹¤μ¤‘ LLM μ§€μ›**: Ollama(λ΅μ»¬) / OpenAI(ν΄λΌμ°λ“) κ°„ λ™μ  μ „ν™
- π­ **Factory ν¨ν„΄**: λ°νƒ€μ„μ— LLM μ κ³µμ μ„ νƒ
- π” **MCP ν”„λ΅ν† μ½**: Model Context Protocol κΈ°λ° ν΄ μ„λ²„
- π§© **λ¨λ“ν• μ•„ν‚¤ν…μ²**: λμ¨ν• κ²°ν•©(Loose Coupling)κ³Ό λ†’μ€ μ‘μ§‘λ„(High Cohesion)
- π¨ **Streamlit UI**: μ›Ή κΈ°λ° μΈν„°λ™ν‹°λΈ μΈν„°νμ΄μ¤

## β¨ μ£Όμ” νΉμ§•

### SOLID μ›μΉ™ μ™„λ²½ μ μ©
| μ›μΉ™ | μ μ© μ‚¬λ΅€ |
|------|-----------|
| **S**ingle Responsibility | κ° ν΄λμ¤λ” ν•λ‚μ μ±…μ„λ§ (LLM, Agent, UI λ¶„λ¦¬) |
| **O**pen/Closed | Factoryλ¥Ό ν†µν• ν™•μ¥ (μƒ LLM μ¶”κ°€ μ‹ κΈ°μ΅΄ μ½”λ“ μμ • λ¶ν•„μ”) |
| **L**iskov Substitution | λ¨λ“  LLMμ€ BaseLLMμΌλ΅ μΉν™ κ°€λ¥ |
| **I**nterface Segregation | ν΄λΌμ΄μ–ΈνΈλ” ν•„μ”ν• μΈν„°νμ΄μ¤λ§ μμ΅΄ |
| **D**ependency Inversion | κµ¬μ²΄ ν΄λμ¤κ°€ μ•„λ‹ μ¶”μƒν™”(BaseLLM)μ— μμ΅΄ |

### μ§€μ› LLM μ κ³µμ
- β… **Ollama**: λ΅μ»¬ LLM (λ¬΄λ£, ν”„λΌμ΄λ²„μ‹ λ³΄μ¥)
  - llama2, llama3, gemma, mistral λ“±
- β… **OpenAI**: ν΄λΌμ°λ“ LLM (API ν‚¤ ν•„μ”)
  - gpt-4, gpt-3.5-turbo λ“±
- β… **vLLM(μ™Έλ¶€ μ„λ²„)**: OpenAI νΈν™ μ—”λ“ν¬μΈνΈ λλ” μ „μ© `/generate` μ—”λ“ν¬μΈνΈ
- π” **ν™•μ¥ κ°€λ¥**: Anthropic Claude, Google Gemini λ“± μ¶”κ°€ κ°€λ¥

## ν΄λ”/νμΌ κµ¬μ΅°
```
project/
β”β”€ llm/
β”‚   β”β”€ __init__.py         # λ¨λ“ μΈν„°νμ΄μ¤
β”‚   β”β”€ base_llm.py         # LLM μ¶”μƒ κΈ°λ° ν΄λμ¤ (ABC)
β”‚   β”β”€ ollama.py           # Ollama LLM κµ¬ν„
β”‚   β”β”€ openai_llm.py       # OpenAI LLM κµ¬ν„
β”‚   β”β”€ vllm_llm.py         # vLLM LLM κµ¬ν„ (μ™Έλ¶€ μ„λ²„ νΈμ¶)
β”‚   β”β”€ factory.py          # LLM Factory ν¨ν„΄
β”‚   β””β”€ example_usage.py    # μ‚¬μ© μμ 
β”β”€ agent/
β”‚   β””β”€ memory_agent.py     # λ©”λ¨λ¦¬ κΈ°λ° μ—μ΄μ „νΈ
β”β”€ ui/
β”‚   β””β”€ streamlit_ui.py     # Streamlit UI
β”β”€ mcp-server/
β”‚   β”β”€ server/
β”‚   β”‚   β””β”€ main.py         # MCP μ„λ²„ (pay ν΄)
β”‚   β”β”€ client/
β”‚   β”‚   β””β”€ main.py         # MCP ν΄λΌμ΄μ–ΈνΈ
β”‚   β””β”€ README.md           # MCP μ‹¤ν–‰ κ°€μ΄λ“
β”β”€ main.py                 # μ „μ²΄ μ΅°λ¦½ λ° μ‹¤ν–‰ μ—”νΈλ¦¬ν¬μΈνΈ
β”β”€ requirements.txt        # μμ΅΄μ„± λ©λ΅
β”β”€ RULES.md                # ν”„λ΅μ νΈ κ°λ° κ·μΉ™(SOLID λ“±)
β””β”€ README.md               # (μ΄ λ¬Έμ„)
```

## π€ λΉ λ¥Έ μ‹μ‘

### 1οΈβƒ£ ν•„μ ν¨ν‚¤μ§€ μ„¤μΉ
```bash
pip install -r requirements.txt
```

### 2οΈβƒ£ LLM μ κ³µμ μ„ νƒ λ° μ‹¤ν–‰

<details open>
<summary><b>Option A: Ollama μ‚¬μ© (λ΅μ»¬ LLM, κ¶μ¥)</b></summary>

**μ¥μ **: λ¬΄λ£, λΉ λ¦„, ν”„λΌμ΄λ²„μ‹ λ³΄μ¥

#### μ„¤μΉ (μµμ΄ 1ν)
```bash
# Ollama μ„¤μΉ: https://ollama.ai/download

# λ¨λΈ λ‹¤μ΄λ΅λ“
ollama pull gemma3n:e4b
# λλ”
ollama pull llama2
```

#### μ‹¤ν–‰
```bash
# ν„°λ―Έλ„ 1: Ollama μ„λ²„ μ‹¤ν–‰
ollama serve

# ν„°λ―Έλ„ 2: Streamlit UI μ‹¤ν–‰
streamlit run main.py
```

</details>

<details>
<summary><b>Option B: OpenAI μ‚¬μ© (ν΄λΌμ°λ“ LLM)</b></summary>

**μ¥μ **: κ°•λ ¥ν• μ„±λ¥, μ„¤μΉ λ¶ν•„μ”  
**λ‹¨μ **: API ν‚¤ ν•„μ”, λΉ„μ© λ°μƒ

#### Linux / macOS
```bash
export OPENAI_API_KEY='sk-your-api-key-here'
export LLM_PROVIDER='openai'
export LLM_MODEL='gpt-4'

streamlit run main.py
```

#### Windows (PowerShell)
```powershell
$env:OPENAI_API_KEY='sk-your-api-key-here'
$env:LLM_PROVIDER='openai'
$env:LLM_MODEL='gpt-4'

streamlit run main.py
```

</details>

<details>
<summary><b>Option C: μ™Έλ¶€ vLLM μ„λ²„ μ‚¬μ© (URL κΈ°λ° μ„λΉ™)</b></summary>

**μ¥μ **: μ™Έλ¶€ νΈμ¤ν…μ μ¥μ , ν™•μ¥/λ°°ν¬ μ©μ΄

#### 1) .env μ‚¬μ© (κ¶μ¥)
```plaintext
# vLLM μ„λ²„ URL (μ: https://your-vllm-server-url.com)
LLM_PROVIDER=vllm
VLLM_SERVER_URL=https://your-vllm-server-url.com
```

#### 2) ν™κ²½λ³€μ μ§μ ‘ μ„¤μ •
```bash
# Linux / macOS
export LLM_PROVIDER='vllm'
export VLLM_SERVER_URL='https://your-vllm-server-url.com'

streamlit run main.py
```

```powershell
# Windows (PowerShell)
$env:LLM_PROVIDER='vllm'
$env:VLLM_SERVER_URL='https://your-vllm-server-url.com'

streamlit run main.py
```

μ°Έκ³ :
- vLLM μ„λ²„κ°€ μμ²΄μ„λ… μΈμ¦μ„μΈ κ²½μ°, μ½”λ“ λ λ²¨μ—μ„ `verify=False` μ„¤μ •μ΄ ν•„μ”ν•  μ μμµλ‹λ‹¤.
- μ„λ²„μ μ—”λ“ν¬μΈνΈκ°€ OpenAI νΈν™ λ² μ΄μ¤ URLμ΄ μ•„λ‹ μ „μ© `/generate` λΌμ°νΈλ¥Ό μ‚¬μ©ν•λ” κ²½μ°μ—λ„ `VLLM_SERVER_URL`λ§ μ¬λ°”λ¥΄κ² μ§€μ •ν•λ©΄ λ™μ‘ν•©λ‹λ‹¤.
</details>

### 3οΈβƒ£ μ›Ή λΈλΌμ°μ € μ ‘μ†
λΈλΌμ°μ €κ°€ μλ™μΌλ΅ μ—΄λ¦¬κ±°λ‚, μλ™μΌλ΅ μ ‘μ†:
```
http://localhost:8501
```

### 4οΈβƒ£ MCP μ„λ²„ ν…μ¤νΈ (μ„ νƒμ‚¬ν•­)
```bash
cd mcp-server
python client/main.py
```
π“– μμ„Έν• λ‚΄μ©: [mcp-server/README.md](mcp-server/README.md)

---

## π“¦ ν™κ²½λ³€μ μ„¤μ •

.env νμΌμ„ μ‚¬μ©ν•μ—¬ ν™κ²½λ³€μλ¥Ό μ„¤μ •ν•©λ‹λ‹¤. `example.env` νμΌμ„ μ°Έκ³ ν•μ—¬ ν•„μ”ν• λ³€μλ¥Ό μ„¤μ •ν•μ„Έμ”.

```plaintext
# LLM κΈ°λ³Έ μ„¤μ •
LLM_PROVIDER=ollama
LLM_MODEL=gemma3n:e4b

# OpenAI μ‚¬μ© μ‹
OPENAI_API_KEY=your-openai-api-key-here

# vLLM μ™Έλ¶€ μ„λ²„ μ‚¬μ© μ‹
VLLM_SERVER_URL=https://your-vllm-server-url.com

# LangSmith νΈλ μ΄μ‹± (μ„ νƒ)
LANGSMITH_TRACING=true
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=
LANGCHAIN_PROJECT=langchain_study
```

μ¶”κ°€ ν:
- UI λ””λ²„κ·Έ λ΅κ·Έ ν‘μ‹λ¥Ό μ›ν•λ©΄ `DEBUG_PROMPT=true`λ¥Ό μ„¤μ •ν•μ„Έμ”. (μ½μ†”μ— ν”„λ΅¬ν”„νΈ νμ¤ν† λ¦¬ μ¶λ ¥)
- vLLM μ„λ²„κ°€ μμ²΄μ„λ… μΈμ¦μ„λΌλ©΄ `LLMFactory.create('vllm', ..., verify=False)`λ΅ μ‹¤ν–‰ν•κ±°λ‚ μ„λ²„ μΈμ¦μ„λ¥Ό μ‹ λΆ° μ €μ¥μ†μ— μ„¤μΉν•μ„Έμ”.

---

## π—οΈ μ•„ν‚¤ν…μ² λ‹¤μ΄μ–΄κ·Έλ¨

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                     Streamlit UI                        β”‚
β”‚                   (streamlit_ui.py)                     β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                     β”‚
                     β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                    Memory Agent                          β”‚
β”‚                  (memory_agent.py)                       β”‚
β”‚         - ConversationBufferMemory                       β”‚
β”‚         - ConversationChain                              β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                     β”‚
                     β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                   LLM Factory                            β”‚
β”‚                   (factory.py)                           β”‚
β”‚         - create(provider, model)                        β”‚
β”‚         - register(provider, class)                      β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
           β”‚                      β”‚
           β–Ό                      β–Ό
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚   OllamaLLM      β”‚    β”‚   OpenAILLM      β”‚
β”‚  (ollama.py)     β”‚    β”‚ (openai_llm.py)  β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
           β”‚                      β”‚
           β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
                      β”‚
                      β–Ό
           β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
           β”‚    BaseLLM       β”‚
           β”‚  (base_llm.py)   β”‚
           β”‚   <<abstract>>   β”‚
           β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
```

## π§­ Mermaid ν΄λμ¤ κµ¬μ„±λ„

```mermaid
classDiagram
    class BaseLLM {
      - model: str
      - config: dict
      + __call__(...)
      + as_langchain_model()
    }

    class OllamaLLM {
      + _initialize()
      + __call__(...)
    }
    BaseLLM <|-- OllamaLLM

    class OpenAILLM {
      + _initialize()
      + __call__(...)
    }
    BaseLLM <|-- OpenAILLM

    class VLLMLLM {
      + _initialize()
      + __call__(prompt)
    }
    BaseLLM <|-- VLLMLLM

    class LLMFactory {
      + create(provider, model, **kwargs) BaseLLM
      + register(provider, cls)
      + get_available_providers() list~str~
    }

    class MemoryAgent {
      - memory: ConversationBufferMemory
      - chain: ConversationChain
      + chat(user_input) str
    }

    class StreamlitUI {
      + run()
    }

    LLMFactory ..> BaseLLM : creates
    MemoryAgent --> BaseLLM : uses (as LangChain model)
    StreamlitUI ..> MemoryAgent : holds
```

## π’΅ κ°λ° κ·μΉ™

### SOLID μ›μΉ™ μ¤€μ
- μμ„Έν• λ‚΄μ©: [RULES.md](RULES.md)
- λ¨λ“  μ½”λ“ μ„¤κ³„ λ° κµ¬ν„ μ‹ SOLID μ›μΉ™μ„ λ°λ“μ‹ μ¤€μ
- κ° λ¨λ“/ν΄λμ¤/ν•¨μλ” **ν•λ‚μ μ±…μ„**λ§ κ°–λ„λ΅ λ¶„λ¦¬
- **ν™•μ¥μ— κ°λ°©**, **μμ •μ— νμ‡„**
- μ¶”μƒν™”μ™€ μμ΅΄μ„± μ—­μ „μ„ μ κ·Ή ν™μ©

### μ½”λ“ μ¤νƒ€μΌ
- Python PEP 8 μ¤€μ
- νƒ€μ… ννΈ μ‚¬μ© κ¶μ¥
- Docstring μ‘μ„± (Google Style)

## LLM μ¶”μƒν™” μ•„ν‚¤ν…μ²

### SOLID μ›μΉ™ μ μ©
```python
# BaseLLM (μ¶”μƒ κΈ°λ° ν΄λμ¤) - μμ΅΄μ„± μ—­μ „ μ›μΉ™
from llm import LLMFactory

# Factory ν¨ν„΄μΌλ΅ LLM μƒμ„± - κ°λ°©/νμ‡„ μ›μΉ™
llm = LLMFactory.create('ollama', 'llama2')
# λλ”
llm = LLMFactory.create('openai', 'gpt-4', temperature=0.7)

# λ¨λ“  LLMμ€ λ™μΌν• μΈν„°νμ΄μ¤ - λ¦¬μ¤μ½”ν”„ μΉν™ μ›μΉ™
response = llm([HumanMessage(content="Hello")])
```

### μƒλ΅μ΄ LLM μ κ³µμ μ¶”κ°€ λ°©λ²•
```python
from llm import BaseLLM, LLMFactory

class CustomLLM(BaseLLM):
    def _initialize(self):
        # μ΄κΈ°ν™” λ΅μ§
        pass
    
    def __call__(self, *args, **kwargs):
        # νΈμ¶ λ΅μ§
        pass

# λ“±λ΅
LLMFactory.register('custom', CustomLLM)

# μ‚¬μ©
llm = LLMFactory.create('custom', 'model-name')
```

---

## π§ ν…μ¤νΈ

### κµ¬μ΅° ν…μ¤νΈ (LLM νΈμ¶ μ—†μ)
```bash
python test_llm.py
```

### MCP μ„λ²„ ν…μ¤νΈ
```bash
cd mcp-server
python client/main.py
```

---

## π”§ λ¬Έμ  ν•΄κ²°

<details>
<summary><b>Ollama μ—°κ²° μ‹¤ν¨</b></summary>

**μ¦μƒ**: `Connection error` λλ” `Failed to connect to Ollama`

**ν•΄κ²°λ°©λ²•**:
```bash
# Ollama μ„λ²„κ°€ μ‹¤ν–‰ μ¤‘μΈμ§€ ν™•μΈ
ollama serve

# λ¨λΈμ΄ λ‹¤μ΄λ΅λ“λμ–΄ μλ”μ§€ ν™•μΈ
ollama list

# λ¨λΈ λ‹¤μ΄λ΅λ“
ollama pull gemma3n:e4b
```
</details>

<details>
<summary><b>OpenAI API ν‚¤ μ¤λ¥</b></summary>

**μ¦μƒ**: `AuthenticationError` λλ” `Invalid API key`

**ν•΄κ²°λ°©λ²•**:
```bash
# API ν‚¤κ°€ μ¬λ°”λ¥΄κ² μ„¤μ •λμ—λ”μ§€ ν™•μΈ
echo $OPENAI_API_KEY  # Linux/macOS
echo $env:OPENAI_API_KEY  # Windows PowerShell

# API ν‚¤ μ¬μ„¤μ •
export OPENAI_API_KEY='sk-your-correct-api-key'
```
</details>

<details>
<summary><b>ν¨ν‚¤μ§€ μ„¤μΉ μ¤λ¥</b></summary>

**μ¦μƒ**: `ModuleNotFoundError` λλ” μμ΅΄μ„± μ¶©λ

**ν•΄κ²°λ°©λ²•**:
```bash
# κ°€μƒν™κ²½ μƒμ„± λ° ν™μ„±ν™” (κ¶μ¥)
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate  # Windows

# ν¨ν‚¤μ§€ μ¬μ„¤μΉ
pip install --upgrade pip
pip install -r requirements.txt
```
</details>

<details>
<summary><b>Streamlit ν¬νΈ μ¶©λ</b></summary>

**μ¦μƒ**: `Address already in use: 8501`

**ν•΄κ²°λ°©λ²•**:
```bash
# λ‹¤λ¥Έ ν¬νΈ μ‚¬μ©
streamlit run main.py --server.port 8502
```
</details>

---

## π€ ν™•μ¥ κ°€μ΄λ“

### μƒλ΅μ΄ LLM μ κ³µμ μ¶”κ°€
```python
# llm/anthropic_llm.py
from llm import BaseLLM, LLMFactory
from langchain_anthropic import ChatAnthropic

class AnthropicLLM(BaseLLM):
    def _initialize(self):
        self.llm = ChatAnthropic(model=self.model, **self.config)
    
    def __call__(self, *args, **kwargs):
        return self.llm(*args, **kwargs)

# λ“±λ΅
LLMFactory.register('anthropic', AnthropicLLM)
```

### μ»¤μ¤ν…€ Agent μ¶”κ°€
```python
# agent/custom_agent.py
from langchain.agents import AgentExecutor

class CustomAgent:
    def __init__(self, llm, tools):
        # Agent λ΅μ§ κµ¬ν„
        pass
```

### μ¶”κ°€ UI ν”„λ μ„μ›ν¬
- **FastAPI**: REST API μ„λ²„ κµ¬μ¶•
- **Gradio**: λΉ λ¥Έ ν”„λ΅ν† νƒ€μ΄ν•‘
- **Flask**: κ²½λ‰ μ›Ή μ„λ²„

---

## π“ κΈ°μ  μ¤νƒ

| μΉ΄ν…κ³ λ¦¬ | κΈ°μ  | μ©λ„ |
|---------|------|------|
| **LLM** | Ollama, OpenAI | λ΅μ»¬/ν΄λΌμ°λ“ μ–Έμ–΄ λ¨λΈ |
| **Framework** | LangChain | LLM μ• ν”λ¦¬μΌ€μ΄μ… ν”„λ μ„μ›ν¬ |
| **Protocol** | MCP | Model Context Protocol |
| **UI** | Streamlit | μ›Ή μΈν„°νμ΄μ¤ |
| **Pattern** | Factory, ABC | λ””μμΈ ν¨ν„΄ |
| **Architecture** | SOLID | μ†ν”„νΈμ›¨μ–΄ μ„¤κ³„ μ›μΉ™ |

---

## π“„ λΌμ΄μ„ μ¤

MIT License - μμ λ΅­κ² μ‚¬μ©, μμ •, λ°°ν¬ κ°€λ¥

---

## π¤ κΈ°μ—¬ν•κΈ°

μ΄μ, PR, ν”Όλ“λ°± ν™μν•©λ‹λ‹¤!

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

**Made with β¤οΈ using SOLID principles** 