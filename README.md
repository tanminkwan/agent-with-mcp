# ğŸ¤– SOLID ê¸°ë°˜ Multi-LLM ì±—ë´‡ í”„ë ˆì„ì›Œí¬

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-green.svg)](https://www.langchain.com/)
[![SOLID](https://img.shields.io/badge/Design-SOLID-orange.svg)](https://en.wikipedia.org/wiki/SOLID)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ ê°œìš”

**SOLID ì›ì¹™**ì„ ì¤€ìˆ˜í•œ í™•ì¥ ê°€ëŠ¥í•œ ëŒ€í™”í˜• ì±—ë´‡ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. í•µì‹¬ì€ **Graph-driven ì•„í‚¤í…ì²˜**ë¡œ, ëŒ€í™” íë¦„ì„ ê·¸ë˜í”„ë¡œ ì •ì˜í•˜ê³ , ê° ë…¸ë“œê°€ ìì‹ ì˜ LLMì„ **.env + LLMFactory**ë¡œ ë…ë¦½ ì„ íƒí•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- ğŸ•¸ï¸ **Graph-driven**: ëŒ€í™”/ì—…ë¬´ íë¦„ì„ ê·¸ë˜í”„ë¡œ ì •ì˜í•˜ê³  ì‹¤í–‰
- ğŸ§© **ë…¸ë“œ ë‹¨ìœ„ LLM ì„ íƒ**: ê° ë…¸ë“œê°€ .env êµ¬ì„±ì— ë”°ë¼ LLMì„ ììœ¨ ì„ íƒ
- ğŸ­ **Factory íŒ¨í„´**: LLM ìƒì„± ì±…ì„ì„ íŒ©í† ë¦¬ë¡œ ë¶„ë¦¬í•˜ì—¬ OCP/DIP ì¤€ìˆ˜
- ğŸ”Œ **MCP ì—°ë™**: Model Context Protocol ê¸°ë°˜ ì™¸ë¶€ íˆ´ í˜¸ì¶œ
- ğŸ¨ **Streamlit UI**: ì›¹ ê¸°ë°˜ ì¸í„°ë™í‹°ë¸Œ ì¸í„°í˜ì´ìŠ¤

## âœ¨ ì£¼ìš” íŠ¹ì§•

### SOLID ì›ì¹™ ì™„ë²½ ì ìš©
| ì›ì¹™ | ì ìš© ì‚¬ë¡€ |
|------|-----------|
| **S**ingle Responsibility | ê° í´ë˜ìŠ¤ëŠ” í•˜ë‚˜ì˜ ì±…ì„ë§Œ (LLM, Agent, UI ë¶„ë¦¬) |
| **O**pen/Closed | Factoryë¥¼ í†µí•œ í™•ì¥ (ìƒˆ LLM ì¶”ê°€ ì‹œ ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”) |
| **L**iskov Substitution | ëª¨ë“  LLMì€ BaseLLMìœ¼ë¡œ ì¹˜í™˜ ê°€ëŠ¥ |
| **I**nterface Segregation | í´ë¼ì´ì–¸íŠ¸ëŠ” í•„ìš”í•œ ì¸í„°í˜ì´ìŠ¤ë§Œ ì˜ì¡´ |
| **D**ependency Inversion | êµ¬ì²´ í´ë˜ìŠ¤ê°€ ì•„ë‹Œ ì¶”ìƒí™”(BaseLLM)ì— ì˜ì¡´ |

### ì§€ì› LLM ì œê³µì
- âœ… **Ollama**: ë¡œì»¬ LLM (ë¬´ë£Œ, í”„ë¼ì´ë²„ì‹œ ë³´ì¥)
  - llama2, llama3, gemma, mistral ë“±
- âœ… **OpenAI**: í´ë¼ìš°ë“œ LLM (API í‚¤ í•„ìš”)
  - gpt-4, gpt-3.5-turbo ë“±
- âœ… **vLLM(ì™¸ë¶€ ì„œë²„)**: OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸ ë˜ëŠ” ì „ìš© `/generate` ì—”ë“œí¬ì¸íŠ¸
- ğŸ”œ **í™•ì¥ ê°€ëŠ¥**: Anthropic Claude, Google Gemini ë“± ì¶”ê°€ ê°€ëŠ¥

## í´ë”/íŒŒì¼ êµ¬ì¡°
```
project/
â”œâ”€ llm/
â”‚   â”œâ”€ __init__.py         # ëª¨ë“ˆ ì¸í„°í˜ì´ìŠ¤
â”‚   â”œâ”€ base_llm.py         # LLM ì¶”ìƒ ê¸°ë°˜ í´ë˜ìŠ¤ (ABC)
â”‚   â”œâ”€ ollama.py           # Ollama LLM êµ¬í˜„
â”‚   â”œâ”€ openai_llm.py       # OpenAI LLM êµ¬í˜„
â”‚   â”œâ”€ vllm_llm.py         # vLLM LLM êµ¬í˜„ (ì™¸ë¶€ ì„œë²„ í˜¸ì¶œ)
â”‚   â”œâ”€ factory.py          # LLM Factory íŒ¨í„´
â”‚   â””â”€ example_usage.py    # ì‚¬ìš© ì˜ˆì œ
â”œâ”€ agent/
â”‚   â”œâ”€ memory_agent.py     # ê·¸ë˜í”„ ì£¼ë„ ì—ì´ì „íŠ¸ (GraphInterface ì£¼ì…)
â”‚   â”œâ”€ graphs/
â”‚   â”‚   â”œâ”€ base.py         # GraphInterface (ABC)
â”‚   â”‚   â”œâ”€ factory.py      # Graph ìƒì„±/ì„ íƒ íŒ©í† ë¦¬ (AGENT_GRAPH)
â”‚   â”‚   â””â”€ purchase_graph.py # ì˜ˆì‹œ ê·¸ë˜í”„ êµ¬í˜„(ì„¸ë¶€ ì„¤ëª… ìƒëµ)
â”‚   â””â”€ nodes/
â”‚       â”œâ”€ purchase_nodes.py # ì˜ˆì‹œ ë…¸ë“œ ëª¨ìŒ(ì„¸ë¶€ ì„¤ëª… ìƒëµ)
â”‚       â””â”€ llm_utils.py      # .env ê¸°ë°˜ LLM ìƒì„± ìœ í‹¸(LLMFactory ì‚¬ìš©)
â”œâ”€ ui/
â”‚   â””â”€ streamlit_ui.py     # Streamlit UI
â”œâ”€ mcp-server/
â”‚   â”œâ”€ server/
â”‚   â”‚   â”œâ”€ app.py          # MCP ì„œë²„ ì •ì˜ (íˆ´ ë“±ë¡)
â”‚   â”‚   â””â”€ sse_main.py     # SSE ì„œë²„ ì‹¤í–‰ ì§„ì…ì 
â”‚   â”œâ”€ client/
â”‚   â”‚   â”œâ”€ multi_main.py   # LangGraph/Agent ì˜ˆì œ í´ë¼ì´ì–¸íŠ¸
â”‚   â”‚   â””â”€ utils.py        # í´ë¼ì´ì–¸íŠ¸ ìœ í‹¸
â”‚   â””â”€ README.md           # MCP ì‹¤í–‰ ê°€ì´ë“œ
â”œâ”€ main.py                 # ì „ì²´ ì¡°ë¦½ ë° ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”œâ”€ requirements.txt        # ì˜ì¡´ì„± ëª©ë¡
â”œâ”€ RULES.md                # í”„ë¡œì íŠ¸ ê°œë°œ ê·œì¹™(SOLID ë“±)
â””â”€ README.md               # (ì´ ë¬¸ì„œ)
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1ï¸âƒ£ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ ì‹¤í–‰

í„°ë¯¸ë„ 1: MCP SSE ì„œë²„ ì‹¤í–‰
```bash
python mcp-server/server/sse_main.py
```

í„°ë¯¸ë„ 2: Streamlit UI ì‹¤í–‰
```bash
streamlit run main.py
```

ì•„ë˜ LLM ì„¤ì • ì˜µì…˜ì€ ê° ë…¸ë“œê°€ .envë¥¼ í†µí•´ ìë™ ë°˜ì˜í•©ë‹ˆë‹¤.

<details open>
<summary><b>Option A: Ollama ì‚¬ìš© (ë¡œì»¬ LLM, ê¶Œì¥)</b></summary>

**ì¥ì **: ë¬´ë£Œ, ë¹ ë¦„, í”„ë¼ì´ë²„ì‹œ ë³´ì¥

#### ì„¤ì¹˜ (ìµœì´ˆ 1íšŒ)
```bash
# Ollama ì„¤ì¹˜: https://ollama.ai/download

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull gemma3n:e4b
# ë˜ëŠ”
ollama pull llama2
```

#### ì‹¤í–‰
```bash
# í„°ë¯¸ë„ 1: Ollama ì„œë²„ ì‹¤í–‰
ollama serve

# í„°ë¯¸ë„ 2: Streamlit UI ì‹¤í–‰
streamlit run main.py
```

</details>

<details>
<summary><b>Option B: OpenAI ì‚¬ìš© (í´ë¼ìš°ë“œ LLM)</b></summary>

**ì¥ì **: ê°•ë ¥í•œ ì„±ëŠ¥, ì„¤ì¹˜ ë¶ˆí•„ìš”  
**ë‹¨ì **: API í‚¤ í•„ìš”, ë¹„ìš© ë°œìƒ

#### Linux / macOS
```bash
export OPENAI_API_KEY='sk-your-api-key-here'
export LLM_PROVIDER='openai'
export LLM_MODEL='gpt-4'

streamlit run main.py
```

#### Windows (PowerShell)
```powershell
$env:OPENAI_API_KEY='sk-your-api-key-here'
$env:LLM_PROVIDER='openai'
$env:LLM_MODEL='gpt-4'

streamlit run main.py
```

</details>

<details>
<summary><b>Option C: ì™¸ë¶€ vLLM ì„œë²„ ì‚¬ìš© (URL ê¸°ë°˜ ì„œë¹™)</b></summary>

**ì¥ì **: ì™¸ë¶€ í˜¸ìŠ¤íŒ…ì˜ ì¥ì , í™•ì¥/ë°°í¬ ìš©ì´

#### 1) .env ì‚¬ìš© (ê¶Œì¥)
```plaintext
# vLLM ì„œë²„ URL (ì˜ˆ: https://your-vllm-server-url.com)
LLM_PROVIDER=vllm
VLLM_SERVER_URL=https://your-vllm-server-url.com
```

#### 2) í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì„¤ì •
```bash
# Linux / macOS
export LLM_PROVIDER='vllm'
export VLLM_SERVER_URL='https://your-vllm-server-url.com'

streamlit run main.py
```

```powershell
# Windows (PowerShell)
$env:LLM_PROVIDER='vllm'
$env:VLLM_SERVER_URL='https://your-vllm-server-url.com'

streamlit run main.py
```

ì°¸ê³ :
- vLLM ì„œë²„ê°€ ìì²´ì„œëª… ì¸ì¦ì„œì¸ ê²½ìš°, ì½”ë“œ ë ˆë²¨ì—ì„œ `verify=False` ì„¤ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì„œë²„ì˜ ì—”ë“œí¬ì¸íŠ¸ê°€ OpenAI í˜¸í™˜ ë² ì´ìŠ¤ URLì´ ì•„ë‹Œ ì „ìš© `/generate` ë¼ìš°íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ë„ `VLLM_SERVER_URL`ë§Œ ì˜¬ë°”ë¥´ê²Œ ì§€ì •í•˜ë©´ ë™ì‘í•©ë‹ˆë‹¤.
</details>

### 3ï¸âƒ£ ì›¹ ë¸Œë¼ìš°ì € ì ‘ì†
ë¸Œë¼ìš°ì €ê°€ ìë™ìœ¼ë¡œ ì—´ë¦¬ê±°ë‚˜, ìˆ˜ë™ìœ¼ë¡œ ì ‘ì†:
```
http://localhost:8501
```

### 4ï¸âƒ£ MCP ì„œë²„ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
```bash
cd mcp-server
python client/main.py
```
ğŸ“– ìì„¸í•œ ë‚´ìš©: [mcp-server/README.md](mcp-server/README.md)

---

## ğŸ“¦ í™˜ê²½ë³€ìˆ˜ ì„¤ì •

.env íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. `example.env` íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ í•„ìš”í•œ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

```plaintext
# LLM ê¸°ë³¸ ì„¤ì • (ë…¸ë“œì—ì„œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©)
LLM_PROVIDER=ollama
LLM_MODEL=gemma3n:e4b

# ê·¸ë˜í”„ ì„ íƒ
AGENT_GRAPH=purchase

# OpenAI ì‚¬ìš© ì‹œ
OPENAI_API_KEY=your-openai-api-key-here

# vLLM ì™¸ë¶€ ì„œë²„ ì‚¬ìš© ì‹œ
VLLM_SERVER_URL=https://your-vllm-server-url.com

# LangSmith íŠ¸ë ˆì´ì‹± (ì„ íƒ)
LANGSMITH_TRACING=true
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=
LANGCHAIN_PROJECT=langchain_study

# (ì„ íƒ) ë…¸ë“œë³„ LLM ì˜¤ë²„ë¼ì´ë“œ ì˜ˆì‹œ
# NODE_POLITENESS_PROVIDER=openai
# NODE_POLITENESS_MODEL=gpt-4o-mini
# NODE_AGENT_PROVIDER=ollama
# NODE_AGENT_MODEL=llama3.1
```

ì¶”ê°€ íŒ:
- UI ë””ë²„ê·¸ ë¡œê·¸ í‘œì‹œë¥¼ ì›í•˜ë©´ `DEBUG_PROMPT=true`ë¥¼ ì„¤ì •í•˜ì„¸ìš”. (ì½˜ì†”ì— í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ ì¶œë ¥)
- vLLM ì„œë²„ê°€ ìì²´ì„œëª… ì¸ì¦ì„œë¼ë©´ `LLMFactory.create('vllm', ..., verify=False)`ë¡œ ì‹¤í–‰í•˜ê±°ë‚˜ ì„œë²„ ì¸ì¦ì„œë¥¼ ì‹ ë¢° ì €ì¥ì†Œì— ì„¤ì¹˜í•˜ì„¸ìš”.

---

## ğŸ—ï¸ Graph-driven ì•„í‚¤í…ì²˜(ê°œìš”)

```mermaid
flowchart TD
    UI[Streamlit UI]
    AGENT[MemoryAgent]
    GFACT[GraphFactory]
    GRAPH[GraphInterface êµ¬í˜„]
    NODES[Nodes]
    LLMF[LLMFactory]

    UI -->|user input| AGENT
    AGENT -->|ainvoke/invoke| GRAPH
    GRAPH --> NODES
    NODES -->|.env ì½ê¸°| LLMF
    LLMF -->|as_langchain_model| NODES
    NODES -->|ë„êµ¬ í˜¸ì¶œ(Optional MCP)| GRAPH
    GRAPH -->|final GraphState| AGENT
    AGENT -->|output í•„ë“œë§Œ ë°˜í™˜| UI
```

## ğŸ§­ ì¸í„°í˜ì´ìŠ¤ì™€ íŒ©í† ë¦¬ í´ë˜ìŠ¤ êµ¬ì¡°

```mermaid
classDiagram
    class GraphInterface {
      + ainvoke(input_text) Dict
      + invoke(input_text) Dict
    }

    class GraphFactory {
      + create(name) GraphInterface
      + create_from_env() GraphInterface
    }

    class MemoryAgent {
      - graph: GraphInterface
      + chat(user_input) str
    }

    class NodeLLMUtils {
      + create_langchain_llm_from_env(prefix) Any
    }

    GraphFactory ..> GraphInterface : returns
    MemoryAgent --> GraphInterface : uses
    GraphInterface ..> NodeLLMUtils : nodes call

    %% LLM ê³„ì¸µ (ë³€ê²½ ì—†ìŒ)
    class BaseLLM {
      - model: str
      - config: dict
      + __call__(...)
      + as_langchain_model()
    }

    class OllamaLLM {
      + _initialize()
      + __call__(...)
    }
    BaseLLM <|-- OllamaLLM

    class OpenAILLM {
      + _initialize()
      + __call__(...)
    }
    BaseLLM <|-- OpenAILLM

    class VLLMLLM {
      + _initialize()
      + __call__(prompt)
    }
    BaseLLM <|-- VLLMLLM

    class LLMFactory {
      + create(provider, model, **kwargs) BaseLLM
      + register(provider, cls)
      + get_available_providers() list~str~
    }
    LLMFactory ..> BaseLLM : creates
```

## ğŸ’¡ ê°œë°œ ê·œì¹™

### SOLID ì›ì¹™ ì¤€ìˆ˜
- ìì„¸í•œ ë‚´ìš©: [RULES.md](RULES.md)
- ëª¨ë“  ì½”ë“œ ì„¤ê³„ ë° êµ¬í˜„ ì‹œ SOLID ì›ì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜
- ê° ëª¨ë“ˆ/í´ë˜ìŠ¤/í•¨ìˆ˜ëŠ” **í•˜ë‚˜ì˜ ì±…ì„**ë§Œ ê°–ë„ë¡ ë¶„ë¦¬
- **í™•ì¥ì— ê°œë°©**, **ìˆ˜ì •ì— íì‡„**
- ì¶”ìƒí™”ì™€ ì˜ì¡´ì„± ì—­ì „ì„ ì ê·¹ í™œìš©

### ì½”ë“œ ìŠ¤íƒ€ì¼
- Python PEP 8 ì¤€ìˆ˜
- íƒ€ì… íŒíŠ¸ ì‚¬ìš© ê¶Œì¥
- Docstring ì‘ì„± (Google Style)

## LLM ì¶”ìƒí™” ê³„ì¸µ(ìš”ì•½)

ë…¸ë“œëŠ” `llm_utils.create_langchain_llm_from_env(prefix)`ë¡œ ìì‹ ì´ ì‚¬ìš©í•  LLMì„ ìƒì„±í•©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ `LLMFactory.create(provider, model, **kwargs)`ë¥¼ í˜¸ì¶œí•˜ì—¬ BaseLLM êµ¬í˜„ì„ ë°˜í™˜í•˜ê³ , í•„ìš”í•œ ê²½ìš° `as_langchain_model()`ì„ í†µí•´ LangChain í˜¸í™˜ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### ìƒˆë¡œìš´ LLM ì œê³µì ì¶”ê°€ ë°©ë²•
```python
from llm import BaseLLM, LLMFactory

class CustomLLM(BaseLLM):
    def _initialize(self):
        # ì´ˆê¸°í™” ë¡œì§
        pass
    
    def __call__(self, *args, **kwargs):
        # í˜¸ì¶œ ë¡œì§
        pass

# ë“±ë¡
LLMFactory.register('custom', CustomLLM)

# ì‚¬ìš©
llm = LLMFactory.create('custom', 'model-name')
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### êµ¬ì¡° í…ŒìŠ¤íŠ¸ (LLM í˜¸ì¶œ ì—†ìŒ)
```bash
python test_llm.py
```

### MCP ì„œë²„ í…ŒìŠ¤íŠ¸
```bash
cd mcp-server
python client/main.py
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

<details>
<summary><b>Ollama ì—°ê²° ì‹¤íŒ¨</b></summary>

**ì¦ìƒ**: `Connection error` ë˜ëŠ” `Failed to connect to Ollama`

**í•´ê²°ë°©ë²•**:
```bash
# Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
ollama serve

# ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
ollama list

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull gemma3n:e4b
```
</details>

<details>
<summary><b>OpenAI API í‚¤ ì˜¤ë¥˜</b></summary>

**ì¦ìƒ**: `AuthenticationError` ë˜ëŠ” `Invalid API key`

**í•´ê²°ë°©ë²•**:
```bash
# API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
echo $OPENAI_API_KEY  # Linux/macOS
echo $env:OPENAI_API_KEY  # Windows PowerShell

# API í‚¤ ì¬ì„¤ì •
export OPENAI_API_KEY='sk-your-correct-api-key'
```
</details>

<details>
<summary><b>íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì˜¤ë¥˜</b></summary>

**ì¦ìƒ**: `ModuleNotFoundError` ë˜ëŠ” ì˜ì¡´ì„± ì¶©ëŒ

**í•´ê²°ë°©ë²•**:
```bash
# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™” (ê¶Œì¥)
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate  # Windows

# íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜
pip install --upgrade pip
pip install -r requirements.txt
```
</details>

<details>
<summary><b>Streamlit í¬íŠ¸ ì¶©ëŒ</b></summary>

**ì¦ìƒ**: `Address already in use: 8501`

**í•´ê²°ë°©ë²•**:
```bash
# ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
streamlit run main.py --server.port 8502
```
</details>

---

## ğŸš€ í™•ì¥ ê°€ì´ë“œ

### ìƒˆë¡œìš´ LLM ì œê³µì ì¶”ê°€
```python
# llm/anthropic_llm.py
from llm import BaseLLM, LLMFactory
from langchain_anthropic import ChatAnthropic

class AnthropicLLM(BaseLLM):
    def _initialize(self):
        self.llm = ChatAnthropic(model=self.model, **self.config)
    
    def __call__(self, *args, **kwargs):
        return self.llm(*args, **kwargs)

# ë“±ë¡
LLMFactory.register('anthropic', AnthropicLLM)
```

### ì»¤ìŠ¤í…€ Agent ì¶”ê°€
```python
# agent/custom_agent.py
from langchain.agents import AgentExecutor

class CustomAgent:
    def __init__(self, llm, tools):
        # Agent ë¡œì§ êµ¬í˜„
        pass
```

### ì¶”ê°€ UI í”„ë ˆì„ì›Œí¬
- **FastAPI**: REST API ì„œë²„ êµ¬ì¶•
- **Gradio**: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
- **Flask**: ê²½ëŸ‰ ì›¹ ì„œë²„

---

## ğŸ“š ê¸°ìˆ  ìŠ¤íƒ

| ì¹´í…Œê³ ë¦¬ | ê¸°ìˆ  | ìš©ë„ |
|---------|------|------|
| **LLM** | Ollama, OpenAI | ë¡œì»¬/í´ë¼ìš°ë“œ ì–¸ì–´ ëª¨ë¸ |
| **Framework** | LangChain | LLM ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë ˆì„ì›Œí¬ |
| **Protocol** | MCP | Model Context Protocol |
| **UI** | Streamlit | ì›¹ ì¸í„°í˜ì´ìŠ¤ |
| **Pattern** | Factory, ABC | ë””ìì¸ íŒ¨í„´ |
| **Architecture** | SOLID | ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ê³„ ì›ì¹™ |

---

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License - ììœ ë¡­ê²Œ ì‚¬ìš©, ìˆ˜ì •, ë°°í¬ ê°€ëŠ¥

---

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

ì´ìŠˆ, PR, í”¼ë“œë°± í™˜ì˜í•©ë‹ˆë‹¤!

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

**Made with â¤ï¸ using SOLID principles** 