#!/usr/bin/env python3
import asyncio
import json
import os
from pathlib import Path
from fastmcp import Client
from typing import Any, Dict, List, TypedDict
from pydantic import create_model
from langchain.tools import StructuredTool
from langchain_openai import ChatOpenAI
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from dotenv import load_dotenv
from langgraph.graph import StateGraph, START, END

# utils ì¬ì‚¬ìš©ì„ ìœ„í•´ ê²½ë¡œ ì¶”ê°€
import sys
sys.path.append(str(Path(__file__).parent))
from utils import print_available_tools

# .envë¥¼ ë¡œë“œí•˜ì—¬ OPENAI_API_KEY / LANGSMITH ê´€ë ¨ í™˜ê²½ë³€ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
load_dotenv()

async def main() -> None:
    # openai_llm.pyì™€ ë™ì¼í•œ ë°©ì‹: ë³„ë„ì˜ .env ë¡œë“œ/í”„ë¡ì‹œ ì¡°ì‘ ì—†ì´ ì‚¬ìš©
    config_path = os.getenv("MCP_SERVERS_CONFIG", str(Path(__file__).parent.parent / "mcp_servers.json"))
    print(f"ğŸ—‚ï¸ ë©€í‹° ì„œë²„ ì„¤ì •: {config_path}")

    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)

    servers = list(config.get("mcpServers", {}).keys())
    print(f"ğŸ”— ëŒ€ìƒ ì„œë²„ë“¤: {', '.join(servers) if servers else '(ì—†ìŒ)'}")
    print()

    async with Client(config) as client:
        # ì „ì²´ ì„œë²„ íˆ´ ì¡°íšŒ(ì„ íƒì  ì¶œë ¥)
        await print_available_tools(client)

        # MCP íˆ´ â†’ LangChain StructuredTool ë³€í™˜ ìœ í‹¸
        def pyd_model_from_json_schema(name: str, schema: Dict[str, Any]):
            schema = schema or {"type": "object", "properties": {}}
            props = schema.get("properties", {})
            req = set(schema.get("required", []))
            def pytype(t: str):
                return {
                    "integer": int,
                    "number": float,
                    "boolean": bool,
                    "object": dict,
                    "array": list,
                }.get(t, str)
            fields = {k: (pytype(v.get("type", "string")), ... if k in req else None) for k, v in props.items()}
            return create_model(name, **fields) if fields else create_model(name)

        def make_langchain_tool(t: Any) -> StructuredTool:
            name = getattr(t, "name", None) or (t.get("name") if isinstance(t, dict) else None)
            desc = getattr(t, "description", "") or (t.get("description") if isinstance(t, dict) else "")
            params = getattr(t, "inputSchema", None) or (t.get("inputSchema") if isinstance(t, dict) else None)
            ArgsModel = pyd_model_from_json_schema(f"{name}_Args", params)

            async def _acall(**kwargs):
                res = await client.call_tool(name, kwargs)
                if hasattr(res, "content") and res.content:
                    texts = [c.text for c in res.content if hasattr(c, "text")]
                    return "\n".join(texts) if texts else str(res)
                return str(res)

            return StructuredTool.from_function(
                name=name,
                description=desc,
                args_schema=ArgsModel,
                coroutine=_acall,
            )

        # MCPì—ì„œ íˆ´ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ í›„ LangChain ë„êµ¬ë¡œ ë³€í™˜
        mcp_tools = await client.list_tools()
        lc_tools = [make_langchain_tool(t) for t in mcp_tools]

        # LangChain ì—ì´ì „íŠ¸ êµ¬ì„±: ëª¨ë¸ì´ ìì—°ì–´ë¡œ íˆ´ ì„ íƒ/í˜¸ì¶œ
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ë„ˆëŠ” ì œê³µëœ MCP íˆ´ ì¤‘ ì ì ˆí•œ ê²ƒì„ ì„ íƒí•´ í˜¸ì¶œí•œë‹¤. í•„ìš” ì—†ìœ¼ë©´ ì§ì ‘ ë‹µí•˜ë¼."),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        agent = create_tool_calling_agent(llm, lc_tools, prompt)
        executor = AgentExecutor(agent=agent, tools=lc_tools, verbose=True, handle_parsing_errors=True)

        # ì˜ˆì˜(ì¡´ëŒ€ë§) ë° ê¸ˆì•¡ ì§€ë¶ˆ ì˜ì‚¬ ë¶„ê¸°ìš© LangGraph êµ¬ì„±
        class GraphState(TypedDict):
            input: str
            is_honorific: bool
            intent_pay_amount: bool
            tool_eligible: bool
            output: str

        async def llm_is_honorific(user_input: str) -> bool:
            instruction = (
                "ì‚¬ìš©ì ë°œí™”ê°€ í•œêµ­ì–´ ì¡´ëŒ€ë§(ë†’ì„ë§)ì¸ì§€ íŒë³„í•˜ë¼. ë°˜ë§ì´ê±°ë‚˜ ì• ë§¤í•˜ë©´ NO, ì¡´ëŒ€ë§ì´ë©´ YES.\n"
                "ë°˜ë“œì‹œ YES ë˜ëŠ” NOë§Œ ì¶œë ¥í•˜ë¼. ë‹¤ë¥¸ ë§ ê¸ˆì§€."
            )
            prompt_text = f"[Instruction]\n{instruction}\n\n[User Input]\n{user_input}"
            resp = await llm.ainvoke(prompt_text)
            content = getattr(resp, "content", "").strip().upper()
            return content.startswith("Y")

        async def llm_has_pay_amount_intent(user_input: str) -> bool:
            instruction = (
                "ë‹¤ìŒ ì‚¬ìš©ì ë°œí™”ì— 'ê¸ˆì•¡ì„ ì§€ë¶ˆ(ê²°ì œ)í•˜ê² ë‹¤ëŠ” ì˜ì‚¬'ê°€ ë¶„ëª…íˆ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ YES,\n"
                "(ìˆ«ìë‚˜ ê¸ˆì•¡ í‘œí˜„ í¬í•¨ ë“±) ê·¸ë ‡ì§€ ì•Šê±°ë‚˜ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ NOë§Œ ì¶œë ¥í•˜ë¼. ë‹¤ë¥¸ ë§ ê¸ˆì§€."
            )
            prompt_text = f"[Instruction]\n{instruction}\n\n[User Input]\n{user_input}"
            resp = await llm.ainvoke(prompt_text)
            content = getattr(resp, "content", "").strip().upper()
            return content.startswith("Y")

        async def llm_is_purchase_tool_available(user_input: str, tools: List[StructuredTool]) -> bool:
            # LLMì—ê²Œ ë„êµ¬ ëª©ë¡ì„ ì£¼ê³ , êµ¬ë§¤/ì£¼ë¬¸/ê²°ì œë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì ì ˆí•œ ë„êµ¬ ì¡´ì¬ ì—¬ë¶€ë¥¼ YES/NOë¡œ íŒë‹¨í•˜ê²Œ í•¨
            tool_lines = []
            for tool in tools:
                name = getattr(tool, "name", "")
                desc = getattr(tool, "description", "")
                tool_lines.append(f"- {name}: {desc}")
            tools_text = "\n".join(tool_lines) if tool_lines else "(no tools)"

            instruction = (
                "ë„ˆëŠ” ì œê³µëœ MCP ë„êµ¬ ëª©ë¡ì„ ë³´ê³ , ì‚¬ìš©ì ìš”ì²­ì´ 'íŠ¹ì • ìƒí’ˆ êµ¬ë§¤/ì£¼ë¬¸/ê²°ì œ'ë¥¼ ì‹¤ì œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë„êµ¬ê°€ ìˆëŠ”ì§€ íŒë‹¨í•œë‹¤.\n"
                "ë„êµ¬ì˜ ì´ë¦„/ì„¤ëª… ìƒ í•´ë‹¹ ì•¡ì…˜ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤ê³  í•©ë¦¬ì ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆì„ ë•Œë§Œ YES, ì•„ë‹ˆë©´ NO.\n"
                "ë°˜ë“œì‹œ ëŒ€ë¬¸ì YES ë˜ëŠ” NOë§Œ ì¶œë ¥í•˜ë¼. ì¶”ê°€ ì„¤ëª… ê¸ˆì§€."
            )
            prompt_text = (
                f"[Instruction]\n{instruction}\n\n"
                f"[User Input]\n{user_input}\n\n"
                f"[Tools]\n{tools_text}\n"
            )
            resp = await llm.ainvoke(prompt_text)
            content = getattr(resp, "content", "").strip().upper()
            return content.startswith("Y")

        async def node_politeness(state: GraphState) -> GraphState:
            text = state["input"]
            is_h = await llm_is_honorific(text)
            return {**state, "is_honorific": is_h}

        def node_polite_warning(state: GraphState) -> GraphState:
            return {**state, "output": "ì¡´ëŒ€ë§ì„ ì¨ì£¼ì„¸ìš”"}

        async def node_classify_pay_amount(state: GraphState) -> GraphState:
            text = state["input"]
            intent = await llm_has_pay_amount_intent(text)
            return {**state, "intent_pay_amount": intent}

        def node_ask_product(state: GraphState) -> GraphState:
            return {
                **state,
                "output": "ì–´ë–¤ ìƒí’ˆì„ êµ¬ë§¤í•˜ì‹œë‚˜ìš”? êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.",
            }

        async def node_check_tool(state: GraphState) -> GraphState:
            try:
                eligible = await llm_is_purchase_tool_available(state["input"], lc_tools)
            except Exception:
                # ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ False ì²˜ë¦¬
                eligible = False
            return {**state, "tool_eligible": eligible}

        async def node_agent(state: GraphState) -> GraphState:
            res = await executor.ainvoke({"input": state["input"]})
            out = res.get("output", "(ì¶œë ¥ ì—†ìŒ)")
            return {**state, "output": out}

        def node_no_tool(state: GraphState) -> GraphState:
            return {**state, "output": "ì ì ˆí•œ tool ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}

        workflow = StateGraph(GraphState)
        workflow.add_node("politeness", node_politeness)
        workflow.add_node("polite_warning", node_polite_warning)
        workflow.add_node("classify_pay_amount", node_classify_pay_amount)
        workflow.add_node("ask_product", node_ask_product)
        workflow.add_node("check_tool", node_check_tool)
        workflow.add_node("agent", node_agent)
        workflow.add_node("no_tool", node_no_tool)

        workflow.add_edge(START, "politeness")
        # ë¶„ê¸° 1: ì¡´ëŒ€ë§ ì—¬ë¶€
        def cond_polite(state: GraphState) -> bool:
            return state.get("is_honorific", False)
        workflow.add_conditional_edges(
            "politeness",
            cond_polite,
            {
                True: "classify_pay_amount",
                False: "polite_warning",
            },
        )
        # ë¶„ê¸° 2: ê¸ˆì•¡ ì§€ë¶ˆ ì˜ì‚¬ ì—¬ë¶€
        def cond_pay_amount(state: GraphState) -> bool:
            return state.get("intent_pay_amount", False)
        workflow.add_conditional_edges(
            "classify_pay_amount",
            cond_pay_amount,
            {
                True: "check_tool",
                False: "ask_product",
            },
        )
        # ë¶„ê¸° 3: ì ì ˆí•œ êµ¬ë§¤ ê´€ë ¨ íˆ´ ì¡´ì¬ ì—¬ë¶€
        def cond_tool(state: GraphState) -> bool:
            return state.get("tool_eligible", False)
        workflow.add_conditional_edges(
            "check_tool",
            cond_tool,
            {
                True: "agent",
                False: "no_tool",
            },
        )
        workflow.add_edge("ask_product", END)
        workflow.add_edge("polite_warning", END)
        workflow.add_edge("agent", END)
        workflow.add_edge("no_tool", END)

        graph = workflow.compile()
        # LangGraph ê·¸ë˜í”„ ì‹œê°í™” ì €ì¥
        try:
            graph_representation = graph.get_graph()
            
            # PNG ì €ì¥ (Mermaid ê¸°ë°˜)
            png_bytes = graph_representation.draw_mermaid_png()
            with open("purchase_flow.png", "wb") as f:
                f.write(png_bytes)
            print("âœ… ê·¸ë˜í”„ ì €ì¥: purchase_flow.png")
            
        except Exception as e:
            print(f"âš ï¸ ê·¸ë˜í”„ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            print("(pillow, pygraphviz ë“±ì˜ íŒ¨í‚¤ì§€ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

        # ì…ë ¥ ì˜ˆì‹œ (í•„ìš” ì‹œ êµì²´)
        user_query = "ëˆ 12345 ì§€ë¶ˆí•´"
        final_state = await graph.ainvoke({"input": user_query, "is_honorific": False, "intent_pay_amount": False, "tool_eligible": False, "output": ""})
        print(final_state.get("output", "(ì¶œë ¥ ì—†ìŒ)"))

        print()
        print("âœ¨ ë©€í‹° ì„œë²„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(main())


