"""
LLM λ¨λ“ κ°„λ‹¨ ν…μ¤νΈ
μ‹¤μ  LLM νΈμ¶ μ—†μ΄ κµ¬μ΅°λ§ ν…μ¤νΈ
"""
from llm import LLMFactory, BaseLLM


def test_factory_available_providers():
    """μ‚¬μ© κ°€λ¥ν• μ κ³µμ λ©λ΅ ν™•μΈ"""
    providers = LLMFactory.get_available_providers()
    print(f"β… μ‚¬μ© κ°€λ¥ν• μ κ³µμ: {providers}")
    assert 'ollama' in providers
    assert 'openai' in providers
    print("   - ollama: OK")
    print("   - openai: OK")


def test_create_ollama():
    """Ollama LLM μƒμ„± ν…μ¤νΈ"""
    print("\nπ§ Ollama LLM μƒμ„± ν…μ¤νΈ")
    llm = LLMFactory.create('ollama', 'llama2', temperature=0.5)
    
    assert isinstance(llm, BaseLLM)
    assert llm.get_model_name() == 'llama2'
    assert llm.get_config()['temperature'] == 0.5
    assert hasattr(llm, 'as_langchain_model')
    
    print(f"   - ν΄λμ¤: {llm.__class__.__name__}")
    print(f"   - λ¨λΈ: {llm.get_model_name()}")
    print(f"   - μ„¤μ •: {llm.get_config()}")
    print("   β… μƒμ„± μ„±κ³µ!")


def test_create_openai():
    """OpenAI LLM μƒμ„± ν…μ¤νΈ"""
    print("\nπ§ OpenAI LLM μƒμ„± ν…μ¤νΈ")
    llm = LLMFactory.create('openai', 'gpt-4', temperature=0.7, max_tokens=100)
    
    assert isinstance(llm, BaseLLM)
    assert llm.get_model_name() == 'gpt-4'
    assert llm.get_config()['temperature'] == 0.7
    assert llm.get_config()['max_tokens'] == 100
    assert hasattr(llm, 'as_langchain_model')
    
    print(f"   - ν΄λμ¤: {llm.__class__.__name__}")
    print(f"   - λ¨λΈ: {llm.get_model_name()}")
    print(f"   - μ„¤μ •: {llm.get_config()}")
    print("   β… μƒμ„± μ„±κ³µ!")


def test_invalid_provider():
    """μλ»λ μ κ³µμλ΅ μƒμ„± μ‹λ„"""
    print("\nπ§ μλ»λ μ κ³µμ ν…μ¤νΈ")
    try:
        LLMFactory.create('invalid', 'model')
        assert False, "μμ™Έκ°€ λ°μƒν•΄μ•Ό ν•¨"
    except ValueError as e:
        print(f"   - μμƒλ μμ™Έ λ°μƒ: {e}")
        print("   β… μμ™Έ μ²λ¦¬ μ„±κ³µ!")


def test_register_custom_llm():
    """μ»¤μ¤ν…€ LLM λ“±λ΅ ν…μ¤νΈ"""
    print("\nπ§ μ»¤μ¤ν…€ LLM λ“±λ΅ ν…μ¤νΈ")
    
    class TestLLM(BaseLLM):
        def _initialize(self):
            self.test_value = "test"
        
        def __call__(self, *args, **kwargs):
            return "test response"
    
    # λ“±λ΅
    LLMFactory.register('test', TestLLM)
    
    # ν™•μΈ
    providers = LLMFactory.get_available_providers()
    assert 'test' in providers
    
    # μƒμ„±
    llm = LLMFactory.create('test', 'test-model')
    assert isinstance(llm, BaseLLM)
    assert llm.get_model_name() == 'test-model'
    
    print(f"   - λ“±λ΅λ μ κ³µμ: {providers}")
    print(f"   - μƒμ„±λ LLM: {llm.__class__.__name__}")
    print("   β… λ“±λ΅ λ° μƒμ„± μ„±κ³µ!")


def test_liskov_substitution():
    """λ¦¬μ¤μ½”ν”„ μΉν™ μ›μΉ™ ν…μ¤νΈ"""
    print("\nπ§ λ¦¬μ¤μ½”ν”„ μΉν™ μ›μΉ™ ν…μ¤νΈ")
    
    # λ¨λ“  LLMμ€ BaseLLMμΌλ΅ μΉν™ κ°€λ¥ν•΄μ•Ό ν•¨
    llms = [
        LLMFactory.create('ollama', 'llama2'),
        LLMFactory.create('openai', 'gpt-4'),
    ]
    
    for llm in llms:
        # λ¨λ“  LLMμ€ λ™μΌν• μΈν„°νμ΄μ¤λ¥Ό κ°€μ Έμ•Ό ν•¨
        assert hasattr(llm, 'get_model_name')
        assert hasattr(llm, 'get_config')
        assert hasattr(llm, 'as_langchain_model')
        assert callable(llm)
        print(f"   - {llm.__class__.__name__}: μΈν„°νμ΄μ¤ νΈν™ β…")
    
    print("   β… λ¨λ“  LLMμ΄ λ™μΌν• μΈν„°νμ΄μ¤ μ κ³µ!")


def main():
    """λ¨λ“  ν…μ¤νΈ μ‹¤ν–‰"""
    print("=" * 60)
    print("π€ LLM λ¨λ“ κµ¬μ΅° ν…μ¤νΈ")
    print("=" * 60)
    
    try:
        test_factory_available_providers()
        test_create_ollama()
        test_create_openai()
        test_invalid_provider()
        test_register_custom_llm()
        test_liskov_substitution()
        
        print("\n" + "=" * 60)
        print("β¨ λ¨λ“  ν…μ¤νΈ ν†µκ³Ό!")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nβ ν…μ¤νΈ μ‹¤ν¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

